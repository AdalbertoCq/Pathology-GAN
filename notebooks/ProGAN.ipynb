{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from preparation.data import Data\n",
    "import h5py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakyReLU(x, alpha):\n",
    "    return tf.maximum(alpha*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to plot number images.\n",
    "def plot_images(plt_num, images, dim):\n",
    "    # Standard parameters for the plot.\n",
    "    \n",
    "    mpl.rcParams[\"figure.figsize\"] = dim, dim\n",
    "    fig = plt.figure()\n",
    "    for i in range(0, plt_num):\n",
    "        fig.add_subplot(1, 10, i+1)\n",
    "        img = images[i, :, :, :]\n",
    "        plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(dataset='nki', marker='he', patch_h=112, patch_w=112, n_channels=3, batch_size=10)\n",
    "images, label = data.training.next_batch(10)\n",
    "\n",
    "plot_images(plt_num=10, images=images, dim=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs(image_width, image_height, image_channels, z_dim):\n",
    "    real_images_56 = tf.placeholder(dtype=tf.float32, shape=(None, image_width, image_height, image_channels), name='real_images_56')\n",
    "    real_images_112 = tf.placeholder(dtype=tf.float32, shape=(None, 2*image_width, 2*image_height, image_channels), name='real_images_112')\n",
    "    z_input_56 = tf.placeholder(dtype=tf.float32, shape=(None, z_dim), name='z_input_56')\n",
    "    z_input_112 = tf.placeholder(dtype=tf.float32, shape=(None, z_dim), name='z_input_112')\n",
    "    learning_rate = tf.placeholder(dtype=tf.float32, name='learning_rate')\n",
    "    return real_images_56, real_images_112, z_input_56, z_input_112, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalization(inputs, normalize):\n",
    "    epsilon = 1e-8\n",
    "    normalization = 1.0\n",
    "    if normalize:\n",
    "        # normalization done accross channels\n",
    "        pixels_mean = tf.reduce_mean(tf.square(inputs), axis=-1, keep_dims=true)\n",
    "        normalization = 1.0/tf.sqrt(pixels_mean + epsilon)\n",
    "    return inputs*normalization\n",
    "\n",
    "\n",
    "# In here we combine Nearest neighbor interpolation and conv, it is faster than upscaling by nearest neirbor \n",
    "# and then conv.\n",
    "def convolutional(inputs, filter_size, output_channels, stride, padding, conv_type, output_shape):\n",
    "    \n",
    "    current_shape = inputs.get_shape()\n",
    "    input_channels = current_shape[3]\n",
    "    \n",
    "    weight_init = tf.contrib.layers.xavier_initializer()\n",
    "    filter_shape = (filter_size, filter_size, input_channels, output_channels)    \n",
    "    filter = tf.get_variable('filter', filter_shape, initializer=weight_init)    \n",
    "    b = tf.get_variable('bias', [1, 1, 1, out_channels], initializer=tf.constant_initializer(0))\n",
    "    \n",
    "    if conv_type == 'upscale':\n",
    "        output_shape = (current_shape[0], current_shape[1]*2, current_shape[2]*2, current_shape[3])\n",
    "        # Weight filter initializer.\n",
    "        filter = tf.pad(filter, ([1,1], [1,1], [0,0], [0,0]), mode='CONSTANT')\n",
    "        filter = tf.add_n([filter[1:,1:], filter[:-1,1:], filter[1:,:-1], filter[:-1,:-1]])\n",
    "        strides = (1, 1, 2, 2)\n",
    "        output = tf.nn.conv2d_transpose(inputs, filter, output_shape, strides, padding)\n",
    "        \n",
    "    elif conv_type == 'downscale':\n",
    "        # Weight filter initializer.\n",
    "        filter = tf.pad(filter, ([1,1], [1,1], [0,0], [0,0]), mode='CONSTANT')\n",
    "        filter = tf.add_n([filter[1:,1:], filter[:-1,1:], filter[1:,:-1], filter[:-1,:-1]])\n",
    "        strides = (1, 1, 2, 2)\n",
    "        output = tf.nn.conv2d(inputs, filter, strides, padding)\n",
    "        \n",
    "    elif conv_type == 'transpose':\n",
    "        strides = (1, 1, stride, stride)\n",
    "        output = tf.nn.conv2d_transpose(inputs, filter, output_shape, strides, padding)\n",
    "    \n",
    "    elif conv_type == 'convolutional':\n",
    "        strides = (1, 1, stride, stride)\n",
    "        output = tf.nn.conv2d(inputs, filter, strides, padding)\n",
    "    \n",
    "    output += b\n",
    "    \n",
    "    return output\n",
    "    \n",
    "\n",
    "def generator_layer(inputs, filter_size, output_channels, stride, padding, alpha, normalize, conv_type, output_shape=None):\n",
    "    # Conv.\n",
    "    net = convolutional_t(inputs, filter_size, output_channels, stride, padding, conv_type, output_shape)\n",
    "\n",
    "    # LReLU or Linear.\n",
    "    if alpha not is None:\n",
    "        net = leakyReLU(net, alpha)\n",
    "\n",
    "    # Pixelwise_norm.\n",
    "    net = feature_normalization(net, normalize)\n",
    "    return net\n",
    "\n",
    "def discriminator_layer(inputs, filter_size, output_channels, stride, padding, alpha, conv_type):\n",
    "    # Conv.\n",
    "    net = convolutional(inputs, filter_size, output_channels, stride, padding, conv_type)\n",
    "\n",
    "    # LReLU or Linear.\n",
    "    if alpha not is None:\n",
    "        net = leakyReLU(net, alpha)\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z_input, num_layers, channels):\n",
    "    with tf.variable_scope('generator'):\n",
    "        with tf.variable_scope('latent'):\n",
    "            net = tf.expand_dims(z_input, 2)\n",
    "            net = tf.expand_dims(z_input, 3)\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            with tf.variable_scope('layer_%s' % i):\n",
    "                # First block layer\n",
    "                if i == num_layers-1:\n",
    "                    net_prev_block = net\n",
    "                elif i == 0:\n",
    "                    # First Conv4 LReLU\n",
    "                    with tf.variable_scope('conv4_1'):\n",
    "                        output_shape = (net.get_shape[0], 4, 4, channels[i])\n",
    "                        net = generator_layer(net, filter_size=4, output_channels=channels[i], stride=1, padding='VALID', \n",
    "                                              alpha=0.2, normalize=True, conv_type='transpose')\n",
    "                    # Second Conv3 LReLU\n",
    "                    with tf.variable_scope('conv3_2'):\n",
    "                        net = generator_layer(net, filter_size=3, output_channels=channels[i], stride=1, padding='SAME', \n",
    "                                              alpha=0.2, normalize=True, conv_type='convolutional')\n",
    "                else:\n",
    "                    # Upsample x2\n",
    "                    with tf.variable_scope('upsample_1'):\n",
    "                        net = generator_layer(net, filter_size=3, output_channels=channels[i], stride=2, padding='SAME', \n",
    "                                              alpha=None, normalize=True, conv_type='upscale')\n",
    "                    # First Conv3 LReLU\n",
    "                    with tf.variable_scope('conv3_2'):\n",
    "                        net = generator_layer(net, filter_size=3, output_channels=channels[i], stride=1, padding='SAME', \n",
    "                                              alpha=0.2, normalize=True, conv_type='convolutional')\n",
    "                    # Second Conv3 LReLU\n",
    "                    with tf.variable_scope('conv3_3'):\n",
    "                        net = generator_layer(net, filter_size=3, output_channels=channels[i], stride=1, padding='SAME', \n",
    "                                              alpha=0.2, normalize=True, conv_type='convolutional')\n",
    "        # Third Conv1 linear.           \n",
    "        with tf.variable_scope('RGB_layer_%s' % i):\n",
    "            net = generator_layer(net, filter_size=1, output_channels=3, stride=1, padding='SAME', \n",
    "                                  alpha=None, normalize=True, conv_type='convolutional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(z_input, num_layers, channels):\n",
    "    with tf.variable_scope('discriminator'):\n",
    "        \n",
    "        # RGB Transition.\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            with tf.variable_scope('layer_%s' % i):\n",
    "                if i==0:\n",
    "                    # Minibatch.\n",
    "                else:\n",
    "                    # First Conv3 LReLU\n",
    "                    with tf.variable_scope('conv3_1'):\n",
    "                        net = discriminator_layer(net, filter_size=3, output_channels=channels[i], stride=1, padding='SAME', \n",
    "                                              alpha=0.2, normalize=True, conv_type='convolutional')\n",
    "                    # Second Conv3 LReLU\n",
    "                    with tf.variable_scope('conv3_2'):\n",
    "                        net = discriminator_layer(net, filter_size=3, output_channels=channels[i], stride=1, padding='SAME', \n",
    "                                              alpha=0.2, normalize=True, conv_type='convolutional')\n",
    "                    # Downsample /2\n",
    "                    with tf.variable_scope('downsample_3'):\n",
    "                        net = discriminator_layer(net, filter_size=3, output_channels=channels[i], stride=2, padding='SAME', \n",
    "                                              alpha=None, conv_type='downscale')\n",
    "                    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_generated(session, output_fake, n_images, z_input_56, z_input_112):\n",
    "    z_dim = z_input_56.get_shape()[-1]\n",
    "    sample_z_56 = np.random.uniform(low=-1., high=1., size=(n_images, z_dim))\n",
    "    sample_z_112 = np.random.uniform(low=-1., high=1., size=(n_images, z_dim))\n",
    "    feed_dict = {z_input_56:sample_z_56, z_input_112:sample_z_112}\n",
    "    gen_samples = session.run(output_fake, feed_dict=feed_dict)\n",
    "    plot_images(plt_num=n_images, images=gen_samples, dim=20)    \n",
    "    return gen_samples, sample_z_56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss(losses, data_out_path, dim):    \n",
    "    mpl.rcParams[\"figure.figsize\"] = dim, dim\n",
    "    plt.rcParams.update({'font.size': 22})\n",
    "    losses = np.array(losses)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(losses[:, 0], label='Discriminator', alpha=0.5)\n",
    "    plt.plot(losses[:, 1], label='Generator', alpha=0.5)\n",
    "    plt.title(\"Training Losses\")\n",
    "    plt.legend()\n",
    "    plt.savefig('%s/training_loss.png' % data_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_output(show_epochs, epochs, data, n_images, image_height, image_width, image_channels, z_dim, data_out_path):\n",
    "    checkpoints_path = os.path.join(data_out_path, 'checkpoints')\n",
    "    checkpoints = os.path.join(checkpoints_path, 'DCGAN.ckt')\n",
    "    gen_images_path = os.path.join(data_out_path, 'images')\n",
    "    gen_images = os.path.join(gen_images_path, 'gen_images.h5')\n",
    "    latent_images = os.path.join(gen_images_path, 'latent_images.h5')\n",
    "    if os.path.isdir(checkpoints_path):\n",
    "         shutil.rmtree(checkpoints_path)\n",
    "    if os.path.isdir(gen_images_path):\n",
    "         shutil.rmtree(gen_images_path)\n",
    "    os.makedirs(checkpoints_path)\n",
    "    os.makedirs(gen_images_path)\n",
    "\n",
    "    #TODO\n",
    "    size_img = (epochs*data[0].training.iterations)//show_epochs\n",
    "    img_db_shape = (size_img, n_images, image_height, image_width, image_channels)\n",
    "    latent_db_shape = (size_img, n_images, z_dim)\n",
    "    hdf5_gen = h5py.File(gen_images, mode='w')\n",
    "    hdf5_latent = h5py.File(latent_images, mode='w')\n",
    "    img_storage = hdf5_gen.create_dataset(name='generated_img', shape=img_db_shape, dtype=np.float32)\n",
    "    latent_storage = hdf5_latent.create_dataset(name='generated_img', shape=latent_db_shape, dtype=np.float32)\n",
    "    return img_storage, latent_storage, checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(real_images_56, real_images_112, z_input_56, z_input_112, out_channel_dim, alpha):\n",
    "    \n",
    "    # Generator.\n",
    "    fake_images_56, fake_images_112 = generator(z_input_56=z_input_56, z_input_112=z_input_112, out_channel_dim=out_channel_dim, \n",
    "                                                reuse=False, is_train=True, alpha=alpha)\n",
    "    \n",
    "    # Discriminator.\n",
    "    output_fake_56, logits_fake_56, output_fake_112, logits_fake_112 = discriminator(images_56=fake_images_56, images_112=fake_images_112, reuse=False, alpha=alpha) \n",
    "    output_real_56, logits_real_56, output_real_112, logits_real_112 = discriminator(images_56=real_images_56, images_112=real_images_112, reuse=True, alpha=alpha)\n",
    "    \n",
    "    '''\n",
    "    Discriminator Losses 56, 112\n",
    "    '''\n",
    "    # Discriminator loss 56.\n",
    "    loss_dis_fake_56 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_fake_56, labels=tf.zeros_like(output_fake_56)))\n",
    "    loss_dis_real_56 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_real_56, labels=tf.ones_like(output_fake_56)*0.9))\n",
    "    loss_dis_56 = loss_dis_fake_56 + loss_dis_real_56\n",
    "    \n",
    "    # Discriminator loss 112.\n",
    "    loss_dis_fake_112 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_fake_112, labels=tf.zeros_like(output_fake_112)))\n",
    "    loss_dis_real_112 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_real_112, labels=tf.ones_like(output_fake_112)*0.9))\n",
    "    loss_dis_112 = loss_dis_fake_112 + loss_dis_real_112\n",
    "\n",
    "    '''\n",
    "    Generator Losses 56, 112\n",
    "    This is where we implement -log[D(G(z))] instead log[1-D(G(z))].\n",
    "    Recall the implementation of cross-entropy, sign already in. \n",
    "    '''\n",
    "    # Generator loss 56.\n",
    "    loss_gen_56 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_fake_56, labels=tf.ones_like(output_fake_56)))\n",
    "\n",
    "    # Generator loss 112.\n",
    "    loss_gen_112 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_fake_112, labels=tf.ones_like(output_fake_112)))\n",
    "    \n",
    "    losses_56 = [loss_dis_56, loss_gen_56, loss_dis_fake_56, loss_dis_real_56]\n",
    "    losses_112 = [loss_dis_112, loss_gen_112, loss_dis_fake_112, loss_dis_real_112]\n",
    "\n",
    "    return losses_56, losses_112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(loss_dis, loss_gen, learning_rate, beta1, level):\n",
    "    trainable_variables = tf.trainable_variables()\n",
    "    \n",
    "    generator_variables_def = [variable for variable in trainable_variables if variable.name.startswith('generator/generator_base')]\n",
    "    generator_variables_res = [variable for variable in trainable_variables if variable.name.startswith('generator/generator_%s'%level)]\n",
    "    generator_variables = generator_variables_def + generator_variables_res\n",
    "    \n",
    "    discriminator_variables_def = [variable for variable in trainable_variables if variable.name.startswith('discriminator/discriminator_base')]\n",
    "    discriminator_variables_res = [variable for variable in trainable_variables if variable.name.startswith('discriminator/discriminator_%s'%level)]\n",
    "    discriminator_variables = discriminator_variables_def + discriminator_variables_res\n",
    "    \n",
    "#     print('Generator_base: ', generator_variables_def)\n",
    "#     print('Generator_res%s: ' % level, generator_variables_res)\n",
    "    \n",
    "#     print('Discriminator_base: ', discriminator_variables_def)\n",
    "#     print('Discriminator_res%s: ' % level, discriminator_variables_res)\n",
    "    \n",
    "    # Handling Batch Normalization.\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_generator = tf.train.AdamOptimizer(learning_rate=learning_rate, \n",
    "                                                 beta1=beta1).minimize(loss_gen, var_list=generator_variables)\n",
    "        train_discriminator = tf.train.AdamOptimizer(learning_rate=learning_rate, \n",
    "                                                     beta1=beta1).minimize(loss_dis, var_list=discriminator_variables)    \n",
    "    return train_generator, train_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size, z_dim, learning_rate, beta1, data, alpha, image_width, image_height, image_channels, data_out_path):\n",
    "    # Placeholder for inputs\n",
    "    real_images_56, real_images_112, z_input_56, z_input_112, learning_rate_input = model_inputs(image_width=image_width, image_height=image_height, \n",
    "                                                                           image_channels=image_channels, z_dim=z_dim)\n",
    "    \n",
    "    # Define losses and model. \n",
    "    losses_56, losses_112 = loss(real_images_56=real_images_56, real_images_112=real_images_112, z_input_56=z_input_56, z_input_112=z_input_112, \n",
    "                                 out_channel_dim=image_channels, alpha=alpha)\n",
    "    loss_dis_56, loss_gen_56, loss_dis_fake_56, loss_dis_real_56 = losses_56\n",
    "    loss_dis_112, loss_gen_112, loss_dis_fake_112, loss_dis_real_112 = losses_112\n",
    "    \n",
    "    # Optimizations for 56 and 112 resolutions.\n",
    "    train_generator_56, train_discriminator_56 = optimization(loss_dis=loss_dis_56, loss_gen=loss_gen_56, learning_rate=learning_rate_input, beta1=beta1,\n",
    "                                                             level='56')\n",
    "    train_generator_112, train_discriminator_112 = optimization(loss_dis=loss_dis_112, loss_gen=loss_gen_112, learning_rate=learning_rate_input, beta1=beta1,\n",
    "                                                               level='112')\n",
    "    \n",
    "    # Generators to produce images.\n",
    "    output_gen_56, output_gen_112 = generator(z_input_56=z_input_56, z_input_112=z_input_112, out_channel_dim=image_channels, reuse=True, is_train=False, alpha=alpha)\n",
    "\n",
    "    global losses_track_56\n",
    "    global losses_track_112\n",
    "        \n",
    "    show_epochs = 100\n",
    "    print_epochs = 10\n",
    "    n_images = 10\n",
    "        \n",
    "    losses_track_56 = list()\n",
    "    losses_track_112 = list()\n",
    "    \n",
    "    data_56, data_112 = data\n",
    "\n",
    "    #Saver for model weights/bias, and generated images.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    img_storage, latent_storage, checkpoints = setup_output(show_epochs, epochs, data, n_images, image_height, image_width, image_channels, z_dim, data_out_path)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        '''\n",
    "        First level training for 56x56x3.\n",
    "        '''\n",
    "        run_epochs = 0\n",
    "        for epoch in range(1, epochs+1):\n",
    "            for batch_images, batch_labels in data_56.training:\n",
    "                z_batch = np.random.uniform(low=-1., high=1., size=(batch_size, z_dim))\n",
    "                real_112 = np.zeros((batch_size, 112, 112, 3))\n",
    "                z_ba_112 = np.zeros((batch_size, z_dim))\n",
    "                feed_dict = {z_input_56:z_batch, z_input_112:z_ba_112, real_images_56:batch_images, real_images_112:real_112, learning_rate_input: learning_rate}\n",
    "                session.run(train_discriminator_56, feed_dict=feed_dict)\n",
    "                session.run(train_generator_56, feed_dict=feed_dict)\n",
    "               \n",
    "                if run_epochs%print_epochs==0:\n",
    "                    feed_dict = {z_input_56:z_batch, z_input_112:z_ba_112, real_images_56:batch_images, real_images_112:real_112}\n",
    "                    epoch_loss_dis = session.run(loss_dis_56, feed_dict=feed_dict)\n",
    "                    epoch_loss_gen = session.run(loss_gen_56, feed_dict=feed_dict)\n",
    "                    losses_track_56.append((epoch_loss_dis, epoch_loss_gen))\n",
    "                    print('Epochs %s/%s: Generator Loss: %s. Discriminator Loss: %s' % (epoch, epochs, np.round(epoch_loss_gen, 3), np.round(epoch_loss_dis, 3)))\n",
    "                if run_epochs%show_epochs == 0:\n",
    "                    gen_samples, sample_z = show_generated(session=session, output_fake=output_gen_56, n_images=n_images, z_input_56=z_input_56, z_input_112=z_input_112)\n",
    "#                     img_storage[run_epochs//show_epochs] = gen_samples\n",
    "#                     latent_storage[run_epochs//show_epochs] = sample_z\n",
    "                    saver.save(sess = session, save_path=checkpoints, global_step=run_epochs)\n",
    "                \n",
    "                run_epochs+=1\n",
    "        '''\n",
    "        Second level training for 112x112x3.\n",
    "        '''\n",
    "        for epoch in range(1, epochs+1):\n",
    "            for batch_images, batch_labels in data_112.training:\n",
    "                z_batch = np.random.uniform(low=-1., high=1., size=(batch_size, z_dim))\n",
    "                real_56 = np.zeros((batch_size, 112, 112, 3))\n",
    "                z_ba_56 = np.zeros((batch_size, z_dim))\n",
    "                feed_dict = {z_input_56:z_ba_56, z_input_112:z_batch, real_images_56:real_56, real_images_112:batch_images, learning_rate_input: learning_rate}\n",
    "                session.run(train_discriminator_112, feed_dict=feed_dict)\n",
    "                session.run(train_generator_112, feed_dict=feed_dict)\n",
    "               \n",
    "                if run_epochs%print_epochs==0:\n",
    "                    feed_dict = {z_input_56:z_ba_56, z_input_112:z_batch, real_images_56:real_56, real_images_112:batch_images}\n",
    "                    epoch_loss_dis = session.run(loss_dis_112, feed_dict=feed_dict)\n",
    "                    epoch_loss_gen = session.run(loss_gen_112, feed_dict=feed_dict)\n",
    "                    losses_track_112.append((epoch_loss_dis, epoch_loss_gen))\n",
    "                    print('Epochs %s/%s: Generator Loss: %s. Discriminator Loss: %s' % (epoch, epochs, np.round(epoch_loss_gen, 3), np.round(epoch_loss_dis, 3)))\n",
    "                if run_epochs%show_epochs == 0:\n",
    "                    gen_samples, sample_z = show_generated(session=session, output_fake=output_gen_56, n_images=n_images, z_input_56=z_input_56, z_input_112=z_input_112)\n",
    "#                     img_storage[run_epochs//show_epochs] = gen_samples\n",
    "#                     latent_storage[run_epochs//show_epochs] = sample_z\n",
    "                    saver.save(sess = session, save_path=checkpoints, global_step=run_epochs)\n",
    "                \n",
    "                run_epochs+=1\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 32\n",
    "z_dim = 100\n",
    "learning_rate = 2e-4\n",
    "beta1 = 0.5\n",
    "alpha = 0.2\n",
    "\n",
    "dataset = 'nki'\n",
    "marker = 'he'\n",
    "image_width = 56\n",
    "image_height = 56\n",
    "image_channels = 3\n",
    "\n",
    "global losses_56\n",
    "global losses_112\n",
    "\n",
    "name_run = 'h%s_w%s_n%s_3iter' % (image_height, image_width, image_channels)\n",
    "data_out_path = '/Users/adalbertoclaudioquiros/Documents/Code/UofG/PhD/Cancer_TMA_Generative/data model output/ProGAN/%s' % name_run\n",
    "\n",
    "data_56 = Data(dataset, marker, patch_h=image_height, patch_w=image_width, n_channels=image_channels, batch_size=batch_size)\n",
    "data_112 = Data(dataset, marker, patch_h=image_height*2, patch_w=image_width*2, n_channels=image_channels, batch_size=batch_size)\n",
    "data = [data_56, data_112]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "   train(epochs, batch_size, z_dim, learning_rate, beta1, data, alpha, image_width, image_height, image_channels, data_out_path)\n",
    "\n",
    "save_loss(losses, data_out_path, dim=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
